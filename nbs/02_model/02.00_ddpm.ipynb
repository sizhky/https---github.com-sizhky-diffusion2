{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model\n",
    "\n",
    "> UNET model for ddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp model.ddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "%reload_ext jupyternotify\n",
    "%reload_ext nb_black\n",
    "%reload_ext autoreload\n",
    "%reload_ext line_profiler\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=\n",
    "\n",
    "import sys\n",
    "\n",
    "__root = \"../../\"\n",
    "sys.path.append(__root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from torch_snippets import *\n",
    "from simple_ddpm.data.load_cifar import denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels=128, size=32):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        self.conv_param = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=128, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv_out = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=128, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "        self.dense_ts = nn.Linear(192, 128)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm([128, size, size])\n",
    "\n",
    "    def forward(self, x_img, x_ts):\n",
    "        x_parameter = F.relu(self.conv_param(x_img))\n",
    "\n",
    "        time_parameter = F.relu(self.dense_ts(x_ts))\n",
    "        time_parameter = time_parameter.view(-1, 128, 1, 1)\n",
    "        x_parameter = x_parameter * time_parameter\n",
    "\n",
    "        x_out = self.conv_out(x_img)\n",
    "        x_out = x_out + x_parameter\n",
    "        x_out = F.relu(self.layer_norm(x_out))\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.l_ts = nn.Sequential(\n",
    "            nn.Linear(1, 192),\n",
    "            nn.LayerNorm([192]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.down_x32 = Block(in_channels=3, size=32)\n",
    "        self.down_x16 = Block(size=16)\n",
    "        self.down_x8 = Block(size=8)\n",
    "        self.down_x4 = Block(size=4)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2240, 128),\n",
    "            nn.LayerNorm([128]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32 * 4 * 4),  # make [-1, 32, 4, 4]\n",
    "            nn.LayerNorm([32 * 4 * 4]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up_x4 = Block(in_channels=32 + 128, size=4)\n",
    "        self.up_x8 = Block(in_channels=256, size=8)\n",
    "        self.up_x16 = Block(in_channels=256, size=16)\n",
    "        self.up_x32 = Block(in_channels=256, size=32)\n",
    "\n",
    "        self.cnn_output = nn.Conv2d(\n",
    "            in_channels=128, out_channels=3, kernel_size=1, padding=0\n",
    "        )\n",
    "\n",
    "        # make optimizer\n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=0.0008)\n",
    "\n",
    "    def forward(self, x, t, y=None):\n",
    "        x_ts = self.l_ts(t)\n",
    "        # ----- left ( down ) -----\n",
    "        blocks = [\n",
    "            self.down_x32,\n",
    "            self.down_x16,\n",
    "            self.down_x8,\n",
    "            self.down_x4,\n",
    "        ]\n",
    "        x_left_layers = []\n",
    "        for i, block in enumerate(blocks):\n",
    "            x = block(x, x_ts)\n",
    "            x_left_layers.append(x)\n",
    "            if i < len(blocks) - 1:\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # ----- MLP -----\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = torch.cat([x, x_ts], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        x = x.view(-1, 32, 4, 4)\n",
    "\n",
    "        # ----- right ( up ) -----\n",
    "        blocks = [\n",
    "            self.up_x4,\n",
    "            self.up_x8,\n",
    "            self.up_x16,\n",
    "            self.up_x32,\n",
    "        ]\n",
    "\n",
    "        for i, block in enumerate(blocks):\n",
    "            # cat left\n",
    "            x_left = x_left_layers[len(blocks) - i - 1]\n",
    "            x = torch.cat([x, x_left], dim=1)\n",
    "\n",
    "            x = block(x, x_ts)\n",
    "            if i < len(blocks) - 1:\n",
    "                x = F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n",
    "\n",
    "        # ----- output -----\n",
    "        x = self.cnn_output(x)\n",
    "        output = {\"output\": x}\n",
    "        if y is not None:\n",
    "            output[\"loss\"] = F.mse_loss(x, y)\n",
    "        return output\n",
    "\n",
    "    def latest_weights_from_checkpoints_folder(\n",
    "        self, folder, prefix=\"model_weights_\", suffix=\".pth\"\n",
    "    ):\n",
    "        chkpts = sorted(\n",
    "            Glob(f\"{folder}/{prefix}*{suffix}\"),\n",
    "            key=lambda x: int(stem(x).replace(prefix, \"\").replace(suffix, \"\")),\n",
    "        )\n",
    "        chkpt = chkpts[-1]\n",
    "        load_torch_model_weights_to(self, chkpt)\n",
    "        return chkpt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def stepwise_denoise(self, n):\n",
    "        o = []\n",
    "        im = torch.randn(\n",
    "            size=(n, 3, self.config.IMG_SIZE, self.config.IMG_SIZE), device=device\n",
    "        )\n",
    "        o.append(im)\n",
    "        for t in range(self.config.timesteps):\n",
    "            _ts = torch.full([n, 1], t, dtype=torch.float)\n",
    "            im = self(im, _ts)[\"output\"]\n",
    "            o.append(im.detach().cpu())\n",
    "        return torch.stack(o).swapaxes(1, 0)\n",
    "\n",
    "    def show_stepwise_denoise(self, n, show_every=4):\n",
    "        o = self.stepwise_denoise(n)\n",
    "        timesteps = self.config.timesteps + 1\n",
    "        o = torch.stack(flatten([_o for _o in o]))\n",
    "        o = denorm(o)\n",
    "        subplots(o, nc=timesteps, sz=(10 * timesteps // 16, 2 * n))\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_ddpm.config.cifar import Config\n",
    "\n",
    "config = Config()\n",
    "model = Model(config)\n",
    "model.show_stepwise_denoise(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_ddpm.config.cifar import Config\n",
    "from simple_ddpm.data.load_cifar import CIFARCars\n",
    "\n",
    "config = Config()\n",
    "config.BATCH_SIZE = 8\n",
    "data = CIFARCars(\"/Users/yeshwanth.y/Downloads/data\", config)\n",
    "dl = data.make_dataloader(collate_function=data.generate_noise_collator)\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdm",
   "language": "python",
   "name": "mdm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
